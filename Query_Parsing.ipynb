{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhSbYX-Oj0_-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForTokenClassification, Trainer, TrainingArguments, GPT2Tokenizer, GPT2LMHeadModel\n",
        "import spacy\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load LLM for embeddings (BERT)\n",
        "tokenizers = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "embedding_model = AutoModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Preprocess query to get embeddings\n",
        "def preprocess_query(query):\n",
        "    inputs = tokenizers(query, return_tensors='pt')\n",
        "    outputs = embedding_model(**inputs)\n",
        "    return outputs.last_hidden_state"
      ],
      "metadata": {
        "id": "4Z355k6-j6QU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_excel('Intent_training_data.xlsx')\n",
        "# data\n",
        "df = pd.DataFrame(data)\n",
        "y_train_intent = df['Label']\n",
        "X_train = df['Intent_Data']"
      ],
      "metadata": {
        "id": "8BCCV-2zj8oT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating embeddings for training\n",
        "X_train_embeddings = [preprocess_query(query).mean(dim=1).detach().numpy()[0] for query in X_train]\n",
        "\n",
        "# Training the intent recognition model\n",
        "intent_model = SVC(probability=True)\n",
        "intent_model.fit(X_train_embeddings, y_train_intent)\n",
        "\n",
        "print(len(X_train_embeddings), len(y_train_intent))\n",
        "print(X_train[249],y_train_intent[249], y_train_intent[498])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOvTaAaxkC-9",
        "outputId": "a0ece768-82f3-4343-d3b0-34ade43df45b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500 500\n",
            "\"Fetch information on HP hardware in Section D\", 0 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Intent recognition using the trained SVC\n",
        "def recognize_intent(query, model):\n",
        "    embedding = preprocess_query(query).mean(dim=1).detach().numpy()[0]\n",
        "    intent_proba = model.predict_proba([embedding])\n",
        "    best_intent_idx = intent_proba.argmax()\n",
        "    best_intent_confidence = intent_proba[0][best_intent_idx]\n",
        "\n",
        "    intent = model.classes_[best_intent_idx]\n",
        "    return intent, best_intent_confidence\n"
      ],
      "metadata": {
        "id": "rOWTBWsQkPgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Token Classification  training\n",
        "# Training examples for Equipment ID\n",
        "Eq_ID = [\n",
        "\n",
        "    \"NOP/LGS/STC/01\", \"Equipment_ID_NOP/LGS/STC/01\", \"ID_NOP/LGS/STC/01\", \"STC/01\",\n",
        "    \"NOP/LGS/STC/02\", \"Equipment_ID_NOP/LGS/STC/02\", \"ID_NOP/LGS/STC/02\", \"STC/02\",\n",
        "    \"NOP/LGS/STC/03\", \"Equipment_ID_NOP/LGS/STC/03\", \"ID_NOP/LGS/STC/03\", \"STC/03\",\n",
        "    \"NOP/LGS/STC/04\", \"Equipment_ID_NOP/LGS/STC/04\", \"ID_NOP/LGS/STC/04\", \"STC/04\",\n",
        "    \"NOP/LGS/STC/05\", \"Equipment_ID_NOP/LGS/STC/05\", \"ID_NOP/LGS/STC/05\", \"STC/05\",\n",
        "    \"NOP/LGS/STC/06\", \"Equipment_ID_NOP/LGS/STC/06\", \"ID_NOP/LGS/STC/06\", \"STC/06\",\n",
        "    \"NOP/LGS/NPRN/01\", \"Equipment_ID_NOP/LGS/NPRN/01\", \"ID_NOP/LGS/NPRN/01\", \"NPRN/01\",\n",
        "    \"NOP/LGS/KVM/01\", \"Equipment_ID_NOP/LGS/KVM/01\", \"ID_NOP/LGS/KVM/01\", \"KVM/01\",\n",
        "    \"NOP/LGS/SW/01\", \"Equipment_ID_NOP/LGS/SW/01\", \"ID_NOP/LGS/SW/01\", \"SW/01\",\n",
        "    \"NOP/LGS/GEI/03\", \"Equipment_ID_NOP/LGS/GEI/03\", \"ID_NOP/LGS/GEI/03\", \"GEI/03\",\n",
        "    \"NOP/LGS/UHF/RX\", \"Equipment_ID_NOP/LGS/UHF/RX\", \"ID_NOP/LGS/UHF/RX\", \"UHF/RX\",\n",
        "    \"NOP/LGS/VHF/TX\", \"Equipment_ID_NOP/LGS/VHF/TX\", \"ID_NOP/LGS/VHF/TX\", \"VHF/TX\",\n",
        "    \"LAN/SW/01\", \"Equipment_ID_LAN/SW/01\", \"ID_LAN/SW/01\", \"SW/01\",\n",
        "    \"SCC_LAN/SW/01\", \"Equipment_ID_SCC_LAN/SW/01\", \"ID_SCC_LAN/SW/01\", \"SW/01\",\n",
        "    \"RKVM/01\", \"Equipment_ID_RKVM/01\", \"ID_RKVM/01\", \"/01\",\n",
        "    \"STATION_GW1\", \"Equipment_ID_STATION_GW1\", \"ID_STATION_GW1\", \"GW1\",\n",
        "    \"NOP/LGS/TH_COM/01\", \"Equipment_ID_NOP/LGS/TH_COM/01\", \"ID_NOP/LGS/TH_COM/01\", \"TH_COM/01\",\n",
        "    \"Replacement_for_STC\", \"Replacement_STC\", \"STC_Replacement\", \"Repl._STC\",\n",
        "    \"NOP/LGS/NRTR/01\", \"Equipment_ID_NOP/LGS/NRTR/01\", \"ID_NOP/LGS/NRTR/01\", \"NRTR/01\",\n",
        "    \"NOP/LGS/SIMPC01\", \"Equipment_ID_NOP/LGS/SIMPC01\", \"ID_NOP/LGS/SIMPC01\", \"SIMPC01\", \"Equipment_ID\"\n",
        "]\n",
        "# Training examples for Name of Equipment\n",
        "Eq_Name = [\n",
        "    \"Station_Computer_01\",\"Station_Comp-00\",\"Station_Computer-01_DELL-T5600\", \"Name_of_Equipment_Station_Computer-01_DELL-T5600\", \"Station_Computer-01\", \"DELL-T5600\",\n",
        "    \"Station_Computer-03_DELL-T5820\", \"Name_of_Equipment_Station_Computer-03_DELL-T5820\", \"Station_Computer-03\", \"DELL-T5820\",\n",
        "    \"GEI_-_SPARE_BECKHOFF/C5101-0010\", \"Name_of_Equipment_GEI_-_SPARE_BECKHOFF/C5101-0010\", \"GEI_-_SPARE_BECKHOFF\", \"C5101-0010\",\n",
        "    \"RKVM-1501_RS_400CD_US\", \"Name_of_Equipment_RKVM-1501_RS_400CD_US\", \"RKVM-1501\", \"RS_400CD_US\",\n",
        "    \"HP_LaserJet_Printer_P3015\", \"Name_of_Equipment_HP_LaserJet_Printer_P3015\", \"HP_LaserJet_Printer\", \"P3015\",\n",
        "    \"NCC_Router-02\", \"Name_of_Equipment_NCC_Router-02\", \"NCC_Router\", \"Router-02\",\n",
        "    \"Cisco-2811\", \"Name_of_Equipment_Cisco-2811\", \"Cisco-2811\", \"Cisco\",\n",
        "    \"CISCO-1760\", \"Name_of_Equipment_CISCO-1760\", \"CISCO-1760\", \"CISCO\",\n",
        "    \"BECKHOFF\", \"Name_of_Equipment_BECKHOFF\", \"BECKHOFF\",\n",
        "    \"RKVM\", \"Name_of_Equipment_RKVM\", \"RKVM\",\n",
        "    \"HP_Compaq_pro_6300\", \"Name_of_Equipment_HP_Compaq_pro_6300\", \"HP_Compaq_pro_6300\", \"HP\",\n",
        "    \"VHF_Computer\", \"Name_of_Equipment_VHF_Computer\", \"VHF_Computer\", \"VHF\",\n",
        "    \"UHF_Computer\", \"Name_of_Equipment_UHF_Computer\", \"UHF_Computer\", \"UHF\",\n",
        "    \"DELL_Precision_5820\", \"Name_of_Equipment_DELL_Precision_5820\", \"DELL_Precision_5820\", \"DELL\",\n",
        "    \"Ethernet_Switch\", \"Name_of_Equipment_Ethernet_Switch\", \"Ethernet_Switch\", \"Switch\",\n",
        "    \"24V/28V_DC_Power_Supply\", \"Name_of_Equipment_24V/28V_DC_Power_Supply\", \"24V/28V_DC_Power_Supply\", \"Power_Supply\",\n",
        "    \"Thinvent_Thinclient\", \"Name_of_Equipment_Thinvent_Thinclient\", \"Thinvent_Thinclient\", \"Thinclient\",\n",
        "    \"Monitor\", \"Name_of_Equipment_Monitor\", \"Monitor\",\n",
        "    \"Station_Computer-04_DELL-T5900\", \"Name_of_Equipment_Station_Computer-04_DELL-T5900\", \"Station_Computer-04\", \"DELL-T5900\",\n",
        "    \"GEI_-_SPARE_BECKHOFF/C5102-0011\", \"Name_of_Equipment_GEI_-_SPARE_BECKHOFF/C5102-0011\", \"GEI_-_SPARE_BECKHOFF\", \"C5102-0011\",\n",
        "    \"RKVM-1502_RS_400CD_US\", \"Name_of_Equipment_RKVM-1502_RS_400CD_US\", \"RKVM-1502\", \"RS_400CD_US\",\n",
        "    \"HP_LaserJet_Printer_P3016\", \"Name_of_Equipment_HP_LaserJet_Printer_P3016\", \"HP_LaserJet_Printer\", \"P3016\",\n",
        "    \"NCC_Router-03\", \"Name_of_Equipment_NCC_Router-03\", \"NCC_Router\", \"Router-03\",\n",
        "    \"Cisco-2812\", \"Name_of_Equipment_Cisco-2812\", \"Cisco-2812\", \"Cisco\",\n",
        "    \"CISCO-1761\", \"Name_of_Equipment_CISCO-1761\", \"CISCO-1761\", \"CISCO\",\n",
        "    \"BECKHOFF-EtherCAT\", \"Name_of_Equipment_BECKHOFF-EtherCAT\", \"BECKHOFF-EtherCAT\", \"BECKHOFF\",\n",
        "    \"RKVM-1503\", \"Name_of_Equipment_RKVM-1503\", \"RKVM-1503\", \"RKVM\",\n",
        "    \"HP_Compaq_pro_6301\", \"Name_of_Equipment_HP_Compaq_pro_6301\", \"HP_Compaq_pro_6301\", \"HP\",\n",
        "    \"VHF_Radio\", \"Name_of_Equipment_VHF_Radio\", \"VHF_Radio\", \"VHF\",\n",
        "    \"UHF_Radio\", \"Name_of_Equipment_UHF_Radio\", \"UHF_Radio\", \"UHF\",\n",
        "    \"DELL_Precision_5830\", \"Name_of_Equipment_DELL_Precision_5830\", \"DELL_Precision_5830\", \"DELL\",\n",
        "    \"Ethernet_Switch_Pro\", \"Name_of_Equipment_Ethernet_Switch_Pro\", \"Ethernet_Switch_Pro\", \"Switch\",\n",
        "    \"24V/28V_AC_Power_Supply\", \"Name_of_Equipment_24V/28V_AC_Power_Supply\", \"24V/28V_AC_Power_Supply\", \"Power_Supply\",\n",
        "    \"Thinclient_Ultra\", \"Name_of_Equipment_Thinclient_Ultra\", \"Thinclient_Ultra\", \"Thinclient\",\n",
        "    \"Large_Monitor\", \"Name_of_Equipment_Large_Monitor\", \"Large_Monitor\", \"Monitor\",\n",
        "]\n",
        "\n",
        "# Training examples for Location (Hardware)\n",
        "Loc_Hardware = [\n",
        "    \"TELEMETRY\", \"Location_TELEMETRY\", \"Located_in_TELEMETRY\", \"TELEMETRY_room\",\n",
        "    \"Comp_Lab\", \"Location_Comp_Lab\", \"Located_in_Comp_Lab\", \"Comp_Lab_area\",\n",
        "    \"EAST_ANT_ROOM\", \"Location_EAST_ANT_ROOM\", \"Located_in_EAST_ANT_ROOM\", \"EAST_ANT_ROOM_section\",\n",
        "    \"WEST_ANT_ROOM\", \"Location_WEST_ANT_ROOM\", \"Located_in_WEST_ANT_ROOM\", \"WEST_ANT_ROOM_zone\",\n",
        "    \"SPARE\", \"Location_SPARE\", \"Located_in_SPARE\", \"SPARE_storage\",\n",
        "    \"IRDCN\", \"Location_IRDCN\", \"Located_in_IRDCN\", \"IRDCN_area\",\n",
        "    \"LAB\", \"Location_LAB\", \"Located_in_LAB\", \"LAB_section\",\n",
        "    \"LK3_ANT_ROOM\", \"Location_LK3_ANT_ROOM\", \"Located_in_LK3_ANT_ROOM\", \"LK3_ANT_ROOM\",\n",
        "    \"IRDCN_ROOM\", \"Location_IRDCN_ROOM\", \"Located_in_IRDCN_ROOM\", \"IRDCN_ROOM_area\",\n",
        "    \"EAST_ANT_ROOM\", \"Location_EAST_ANT_ROOM\", \"EAST_ANT_ROOM_location\", \"East_side_ANT_ROOM\",\n",
        "    \"WEST_ANT_ROOM\", \"Location_WEST_ANT_ROOM\", \"WEST_ANT_ROOM_place\", \"West_side_ANT_ROOM\",\n",
        "    \"TELEMETRY\", \"Location_TELEMETRY\", \"TELEMETRY_location\", \"TELEMETRY_unit\",\n",
        "    \"Comp_Lab\", \"Location_Comp_Lab\", \"Comp_Lab_position\", \"Comp_Lab\",\n",
        "    \"LAB\", \"Location_LAB\", \"LAB_location\", \"LAB\",\n",
        "    \"SPARE\", \"Location_SPARE\", \"SPARE_location\", #\"SPARE\",\n",
        "    \"IRDCN\", \"Location_IRDCN\", \"IRDCN_place\", \"IRDCN_area\",\n",
        "    \"LK3_ANT_ROOM\", \"Location_LK3_ANT_ROOM\", \"LK3_ANT_ROOM_location\", \"LK3_ANT_ROOM\",\n",
        "    \"EAST_ANT_ROOM\", \"Location_EAST_ANT_ROOM\", \"EAST_ANT_ROOM\", \"EAST_ANT_ROOM\",\n",
        "    \"WEST_ANT_ROOM\", \"Location_WEST_ANT_ROOM\", \"WEST_ANT_ROOM_area\", \"WEST_ANT_ROOM\",\n",
        "    \"TELEMETRY\", \"Location_TELEMETRY\", \"TELEMETRY\", \"TELEMETRY\",\n",
        "    \"Comp_Lab\", \"Location_Comp_Lab\", \"Comp_Lab_location\", \"Comp_Lab\",\n",
        "    \"EAST_ANT_ROOM\", \"Location_EAST_ANT_ROOM\", \"EAST_ANT_ROOM_section\", \"EAST_ANT_ROOM\",\n",
        "    \"WEST_ANT_ROOM\", \"Location_WEST_ANT_ROOM\", \"WEST_ANT_ROOM\", \"WEST_ANT_ROOM\",\n",
        "    \"SPARE\", \"Location_SPARE\", \"SPARE_location\", \"SPARE\",\n",
        "    \"IRDCN_ROOM\", \"Location_IRDCN_ROOM\", \"IRDCN_ROOM\", \"IRDCN_ROOM_area\",\n",
        "    \"LAB\", \"Location_LAB\", \"LAB_area\", \"LAB\",\n",
        "    \"LK3_ANT_ROOM\", \"Location_LK3_ANT_ROOM\", \"LK3_ANT_ROOM\", \"LK3_ANT_ROOM_location\",\n",
        "    \"EAST_ANT_ROOM\", \"Location_EAST_ANT_ROOM\", \"EAST_ANT_ROOM_zone\", \"EAST_ANT_ROOM\",\n",
        "    \"WEST_ANT_ROOM\", \"Location_WEST_ANT_ROOM\", \"WEST_ANT_ROOM\", \"WEST_ANT_ROOM\",\n",
        "    \"TELEMETRY\", \"Location_TELEMETRY\", \"TELEMETRY\", \"TELEMETRY\",\n",
        "    \"Comp_Lab\", \"Location_Comp_Lab\", \"Comp_Lab\", \"Comp_Lab\",\n",
        "    \"IRDCN\", \"Location_IRDCN\", \"IRDCN\", \"IRDCN\",\n",
        "    \"SPARE\", \"Location_SPARE\", \"SPARE\", \"SPARE\", #\"Located\", \"Location\", \"Area\", \"Storage\",\n",
        "    \"LAB\", \"Location_LAB\", \"LAB\", \"LAB\",\n",
        "    \"LK3_ANT_ROOM\", \"Location_LK3_ANT_ROOM\", \"LK3_ANT_ROOM\", \"LK3_ANT_ROOM\"\n",
        "]\n",
        "# Training examples for Serial No.\n",
        "S_No = [\n",
        "    \"C1954Y1\", \"Serial_No._C1954Y1\", \"C1954Y1_serial_number\", \"Serial_number_C1954Y1\",\n",
        "    \"81954Y1\", \"Serial_No._81954Y1\", \"81954Y1_serial\", \"Serial_number_81954Y1\",\n",
        "    \"FF2ZQ13\", \"Serial_No._FF2ZQ13\", \"FF2ZQ13_number\", \"Serial_number_FF2ZQ13\",\n",
        "    \"712656.035\", \"Serial_No._712656.035\", \"712656.035_serial\", \"Serial_number_712656.035\",\n",
        "    \"080104034\", \"Serial_No._080104034\", \"080104034_serial\", \"Serial_number_080104034\",\n",
        "    \"VNB8B5YJ2G\", \"Serial_No._VNB8B5YJ2G\", \"VNB8B5YJ2G_number\", \"Serial_number_VNB8B5YJ2G\",\n",
        "    \"FMKD85011UG\", \"Serial_No._FMKD85011UG\", \"FMKD85011UG_serial\", \"Serial_number_FMKD85011UG\",\n",
        "    \"5DR1VG2\", \"Serial_No._5DR1VG2\", \"5DR1VG2_number\", \"Serial_number_5DR1VG2\",\n",
        "    \"22954Y1\", \"Serial_No._22954Y1\", \"22954Y1_serial_number\", \"Serial_number_22954Y1\",\n",
        "    \"INA308X24M\", \"Serial_No._INA308X24M\", \"INA308X24M_number\", \"Serial_number_INA308X24M\",\n",
        "    \"7XY2V72\", \"Serial_No._7XY2V72\", \"7XY2V72_serial\", \"Serial_number_7XY2V72\",\n",
        "    \"BCV2241J0B1\", \"Serial_No._BCV2241J0B1\", \"BCV2241J0B1_number\", \"Serial_number_BCV2241J0B1\",\n",
        "    \"5DRIVG2\", \"Serial_No._5DRIVG2\", \"5DRIVG2_serial\", \"Serial_number_5DRIVG2\",\n",
        "    \"C1954Y1\", \"Serial_No._C1954Y1\", \"C1954Y1\", \"Serial_number_C1954Y1\",\n",
        "    \"81954Y1\", \"Serial_No._81954Y1\", \"81954Y1\", \"Serial_number_81954Y1\",\n",
        "    \"FF2ZQ13\", \"Serial_No._FF2ZQ13\", \"FF2ZQ13\", \"Serial_number_FF2ZQ13\",\n",
        "    \"712656.035\", \"Serial_No._712656.035\", \"712656.035\", \"Serial_number_712656.035\",\n",
        "    \"080104034\", \"Serial_No._080104034\", \"080104034\", \"Serial_number_080104034\",\n",
        "    \"VNB8B5YJ2G\", \"Serial_No._VNB8B5YJ2G\", \"VNB8B5YJ2G\", \"Serial_number_VNB8B5YJ2G\",\n",
        "    \"FMKD85011UG\", \"Serial_No._FMKD85011UG\", \"FMKD85011UG\", \"Serial_number_FMKD85011UG\",\n",
        "    \"5DR1VG2\", \"Serial_No._5DR1VG2\", \"5DR1VG2\", \"Serial_number_5DR1VG2\",\n",
        "    \"22954Y1\", \"Serial_No._22954Y1\", \"22954Y1\", \"Serial_number_22954Y1\",\n",
        "    \"INA308X24M\", \"Serial_No._INA308X24M\", \"INA308X24M\", \"Serial_number_INA308X24M\",\n",
        "    \"7XY2V72\", \"Serial_No._7XY2V72\", \"7XY2V72\", \"Serial_number_7XY2V72\",\n",
        "    \"BCV2241J0B1\", \"Serial_No._BCV2241J0B1\", \"BCV2241J0B1\", \"Serial_number_BCV2241J0B1\",\n",
        "    \"5DRIVG2\", \"Serial_No._5DRIVG2\", \"5DRIVG2\", \"Serial_number_5DRIVG2\",\n",
        "    \"C1954Y1\", \"Serial_No._C1954Y1\", \"C1954Y1\", \"Serial_number_C1954Y1\",\n",
        "    \"81954Y1\", \"Serial_No._81954Y1\", \"81954Y1\", \"Serial_number_81954Y1\",\n",
        "    \"FF2ZQ13\", \"Serial_No._FF2ZQ13\", \"FF2ZQ13\", \"Serial_number_FF2ZQ13\",\n",
        "    \"712656.035\", \"Serial_No._712656.035\", \"712656.035\", \"Serial_number_712656.035\",\n",
        "    \"080104034\", \"Serial_No._080104034\", \"080104034\", \"Serial_number_080104034\",\n",
        "    \"VNB8B5YJ2G\", \"Serial_No._VNB8B5YJ2G\", \"VNB8B5YJ2G\", \"Serial_number_VNB8B5YJ2G\",\n",
        "    \"FMKD85011UG\", \"Serial_No._FMKD85011UG\", \"FMKD85011UG\", \"Serial_number_FMKD85011UG\",\n",
        "    \"5DR1VG2\", \"Serial_No._5DR1VG2\", \"5DR1VG2\", \"Serial_number_5DR1VG2\",\n",
        "    \"22954Y1\", \"Serial_No._22954Y1\", \"22954Y1\", \"Serial_number_22954Y1\",\n",
        "    \"INA308X24M\", \"Serial_No._INA308X24M\", \"INA308X24M\", \"Serial_number_INA308X24M\",\n",
        "    \"7XY2V72\", \"Serial_No._7XY2V72\", \"7XY2V72\", \"Serial_number_7XY2V72\",\n",
        "    \"BCV2241J0B1\", \"Serial_No._BCV2241J0B1\", \"BCV2241J0B1\", \"Serial_number_BCV2241J0B1\",\n",
        "    \"5DRIVG2\", \"Serial_No._5DRIVG2\", \"5DRIVG2\", \"Serial_number_5DRIVG2\"\n",
        "]\n",
        "\n",
        "# Training examples for Store Ref. No.\n",
        "SR_No = [\n",
        "    \"ISTRAC/A00020061844\", \"Store_Ref._No._ISTRAC/A00020061844\", \"ISTRAC/A00020061844_ref\", \"Ref._ISTRAC/A00020061844\",\n",
        "    \"ISTRAC/A00020061843\", \"Store_Ref._No._ISTRAC/A00020061843\", \"ISTRAC/A00020061843_ref\", \"Ref._ISTRAC/A00020061843\",\n",
        "    \"ISTRAC/A00020070524\", \"Store_Ref._No._ISTRAC/A00020070524\", \"ISTRAC/A00020070524_ref\", \"Ref._ISTRAC/A00020070524\",\n",
        "    \"ISTRAC/A00020061850\", \"Store_Ref._No._ISTRAC/A00020061850\", \"ISTRAC/A00020061850_ref\", \"Ref._ISTRAC/A00020061850\",\n",
        "    \"ISTRAC/IE/5334\", \"Store_Ref._No._ISTRAC/IE/5334\", \"ISTRAC/IE/5334_ref\", \"Ref._ISTRAC/IE/5334\",\n",
        "    \"ISTRAC/IE/5336\", \"Store_Ref._No._ISTRAC/IE/5336\", \"ISTRAC/IE/5336_ref\", \"Ref._ISTRAC/IE/5336\",\n",
        "    \"ISTRAC/A00020061844\", \"Ref._ISTRAC/A00020061844\", \"Store_ISTRAC/A00020061844\", \"Number_ISTRAC/A00020061844\",\n",
        "    \"ISTRAC/A00020061843\", \"Ref._ISTRAC/A00020061843\", \"Store_ISTRAC/A00020061843\", \"Number_ISTRAC/A00020061843\",\n",
        "    \"ISTRAC/A00020070524\", \"Ref._ISTRAC/A00020070524\", \"Store_ISTRAC/A00020070524\", \"Number_ISTRAC/A00020070524\",\n",
        "    \"ISTRAC/A00020061850\", \"Ref._ISTRAC/A00020061850\", \"Store_ISTRAC/A00020061850\", \"Number_ISTRAC/A00020061850\",\n",
        "    \"ISTRAC/IE/5334\", \"Ref._ISTRAC/IE/5334\", \"Store_ISTRAC/IE/5334\", \"Number_ISTRAC/IE/5334\",\n",
        "    \"ISTRAC/IE/5336\", \"Ref._ISTRAC/IE/5336\", \"Store_ISTRAC/IE/5336\", \"Number_ISTRAC/IE/5336\",\n",
        "    \"Store_Ref._ISTRAC/A00020061844\", \"Store_Ref._ISTRAC/A00020061843\", \"Store_Ref._ISTRAC/A00020070524\",\n",
        "    \"Store_Ref._ISTRAC/A00020061850\", \"Store_Ref._ISTRAC/IE/5334\", \"Store_Ref._ISTRAC/IE/5336\",\n",
        "    \"Store_ISTRAC/A00020061844\", \"Store_ISTRAC/A00020061843\", \"Store_ISTRAC/A00020070524\",\n",
        "    \"Store_ISTRAC/A00020061850\", \"Store_ISTRAC/IE/5334\", \"Store_ISTRAC/IE/5336\",\n",
        "    \"Ref._ISTRAC/A00020061844\", \"Ref._ISTRAC/A00020061843\", \"Ref._ISTRAC/A00020070524\",\n",
        "    \"Ref._ISTRAC/A00020061850\", \"Ref._ISTRAC/IE/5334\", \"Ref._ISTRAC/IE/5336\",\n",
        "    \"Number_ISTRAC/A00020061844\", \"Number_ISTRAC/A00020061843\", \"Number_ISTRAC/A00020070524\",\n",
        "    \"Number_ISTRAC/A00020061850\", \"Number_ISTRAC/IE/5334\", \"Number_ISTRAC/IE/5336\"\n",
        "]\n",
        "\n",
        "# Training examples for Asset ID\n",
        "Asset_ID =[\n",
        "    \"A00020130592\", \"A00020130607\", \"A00020070496\", \"A00020200422\", \"A00020191234\",\n",
        "    \"A00020210011\", \"A00020231456\", \"A00020252000\", \"A00020031234\", \"A00020141122\",\n",
        "    \"A00020152233\", \"A00020214567\", \"A00020163344\", \"A00020225478\", \"A00020071123\",\n",
        "    \"A00020172234\", \"A00020231011\", \"A00020183200\", \"A00020091234\", \"A00020194455\",\n",
        "    \"A00020202345\", \"A00020212567\", \"A00020105678\", \"A00020219876\", \"A00020081234\",\n",
        "    \"A00020113344\", \"A00020124455\", \"A00020232200\", \"A00020090112\", \"A00020134567\",\n",
        "    \"A00020243321\", \"A00020155678\", \"A00020251010\", \"A00020260000\", \"A00020166789\",\n",
        "    \"A00020271111\", \"A00020070809\", \"A00020284456\", \"A00020177890\", \"A00020188899\",\n",
        "    \"A00020295555\", \"A00020100001\", \"A00020111112\", \"A00020090001\", \"A00020212223\",\n",
        "    \"A00020223334\", \"A00020134555\", \"A00020245566\", \"A00020056789\", \"A00020167890\",\n",
        "    \"A00020278899\", \"A00020178901\", \"A00020289900\", \"A00020189012\", \"A00020291234\",\n",
        "    \"A00020200123\", \"A00020011022\", \"A00020122334\", \"A00020233445\", \"A00020045678\",\n",
        "    \"Asset_ID_A00020130592\", \"Asset_ID_A00020130607\", \"Asset_ID_A00020070496\",\n",
        "    \"Asset_ID_A00020200422\", \"Asset_ID_A00020191234\", \"Asset_ID_A00020210011\",\n",
        "    \"Asset_ID_A00020231456\", \"Asset_ID_A00020252000\", \"Asset_ID_A00020031234\",\n",
        "    \"Asset_ID_A00020141122\", \"Asset_ID_A00020152233\", \"Asset_ID_A00020214567\",\n",
        "    \"Asset_ID_A00020163344\", \"Asset_ID_A00020225478\", \"Asset_ID_A00020071123\",\n",
        "    \"Asset_ID_A00020172234\", \"Asset_ID_A00020231011\", \"Asset_ID_A00020183200\",\n",
        "    \"Asset_ID_A00020091234\", \"Asset_ID_A00020194455\", \"Asset_ID_A00020202345\"\n",
        "]\n",
        "\n",
        "# Training examples for Column Name\n",
        "# Col_Name = [\n",
        "#     \"Column_Name_Equipment_ID\", \"Column_Name_Name_of_Equipment\", \"Column_Name_Location\",\n",
        "#     \"Column_Name_Serial_No.\", \"Column_Name_Store_Ref._No.\", \"Column_Name_Asset_ID\",\n",
        "#     \"Column_Name_Software_ID\", \"Column_Name_Software_Name\", \"Column_Name_Size\",\n",
        "#     \"Column_Name_Location_(Software)\", \"Column_Name_Revision_No.\", \"Column_Name_Date_of_Updation\",\n",
        "#     \"Column_Name_Action\", \"Column_Name_General\", \"Column_Name\", \"Column_Name_Equipment_ID\",\n",
        "#     \"Column_Name_Name_of_Equipment\", \"Column_Name_Location_(Hardware)\", \"Column_Name_Serial_No.\",\n",
        "#     \"Column_Name_Store_Ref._No.\", \"Column_Name_Asset_ID\", \"Column_Name_Software_ID\",\n",
        "#     \"Column_Name_Software_Name\", \"Column_Name_Size\", \"Column_Name_Location_(Software)\",\n",
        "#     \"Column_Name_Revision_No.\", \"Column_Name_Date_of_Updation\", \"Column_Name_Action\",\n",
        "#     \"Column_Name_General\", \"Column_Name_Equipment_ID\", \"Column_Name_Name_of_Equipment\",\n",
        "#     \"Column_Name_Location\", \"Column_Name_Serial_No.\", \"Column_Name_Store_Ref._No.\",\n",
        "#     \"Column_Name_Asset_ID\", \"Column_Name_Software_ID\", \"Column_Name_Software_Name\",\n",
        "#     \"Column_Name_Size\", \"Column_Name_Location_(Software)\", \"Column_Name_Revision_No.\",\n",
        "#     \"Column_Name_Date_of_Updation\", \"Column_Name_Action\", \"Column_Name_General\",\n",
        "#     \"Column_Name_Equipment_ID\", \"Column_Name_Name_of_Equipment\", \"Column_Name_Location\",\n",
        "#     \"Column_Name_Serial_No.\", \"Column_Name_Store_Ref._No.\", \"Column_Name_Asset_ID\",\n",
        "#     \"Column_Name_Software_ID\", \"Column_Name_Software_Name\", \"Column_Name_Size\",\n",
        "#     \"Column_Name_Location_(Software)\", \"Column_Name_Revision_No.\", \"Column_Name_Date_of_Updation\",\n",
        "#     \"Column_Name_Action\", \"Column_Name_General\", \"Column_Name_Equipment_ID\",\n",
        "#     \"Column_Name_Name_of_Equipment\", \"Equipment_ID\", \"Name_of_Equipment\", \"Location\", \"Serial_No.\",\n",
        "#     \"Store_Ref._No.\", \"Asset_ID\", \"Software_ID\", \"Software_Name\",\n",
        "#     \"Size\", \"Location_(Software)\", \"Revision_No.\", \"Date_of_Updation\",\n",
        "#     \"Action\", \"General\", \"Equipment_ID\", \"Name_of_Equipment\",\n",
        "#     \"Location_(Hardware)\", \"Serial_No.\", \"Store_Ref._No.\",\n",
        "#     \"Asset_ID\", \"Software_ID\", \"Software_Name\", \"Size\",\n",
        "#     \"Location_(Software)\", \"Revision_No.\", \"Date_of_Updation\",\n",
        "#     \"Action\", \"General\", \"Equipment_ID\", \"Name_of_Equipment\",\n",
        "#     \"Location\", \"Serial_No.\", \"Store_Ref._No.\", \"Asset_ID\",\n",
        "#     \"Software_ID\", \"Software_Name\", \"Size\", \"Location_(Software)\",\n",
        "#     \"Revision_No.\", \"Date_of_Updation\", \"Action\", \"General\",\n",
        "#     \"Equipment_ID\", \"Name_of_Equipment\", \"Location\",\n",
        "#     \"Serial_No.\", \"Store_Ref._No.\", \"Asset_ID\", \"Software_ID\",\n",
        "#     \"Software_Name\", \"Size\", \"Location_(Software)\", \"Revision_No.\",\n",
        "#     \"Date_of_Updation\", \"Action\", \"General\", \"Equipment_ID\", \"Name_of_Equip\", \"Location_Info\", \"Serial_No\",\n",
        "#     \"Store_Ref_Number\", \"Asset_Identifier\", \"Software_ID\", \"Software_Title\",\n",
        "#     \"Software_Size\", \"Software_Location\", \"Revision_Number\", \"Update_Date\",\n",
        "#     \"Action_Taken\", \"General_Info\", \"Equip_ID\", \"Name_of_Equipment\",\n",
        "#     \"Location_(Hardware)\", \"Serial_Number\", \"Store_Reference_No.\",\n",
        "#     \"Asset_Identification\", \"Software_Code\", \"Software_Name\", \"Software_Size\",\n",
        "#     \"Location_of_Software\", \"Revision_Information\", \"Date_Updated\",\n",
        "#     \"Action_Required\", \"General_Details\", \"Equipment_ID\", \"Equipment_Name\",\n",
        "#     \"Location_(H/W)\", \"Serial_Number\", \"Store_Ref_No.\", \"Asset_Code\",\n",
        "#     \"Software_ID\", \"Software_Description\", \"Software_Size\", \"Software_Location\",\n",
        "#     \"Revision_Info\", \"Date_of_Update\", \"Action_Taken\", \"General_Info\",\n",
        "#     \"Equipment_ID\", \"Equipment_Details\", \"Location\", \"Serial_Number\",\n",
        "#     \"Store_Ref._No.\", \"Asset_ID\", \"Software_ID\", \"Software_Title\",\n",
        "#     \"Software_Size\", \"Software_Location\", \"Revision_No.\", \"Update_Date\"\n",
        "# ]\n",
        "\n",
        "# Training examples for General token classification\n",
        "General = [\n",
        "    \"Show\", \"List\", \"Fetch\", \"Retrieve\", \"Display\", \"Find\",\n",
        "    \"Get\", \"Show me\", \"List all\", \"Provide\", \"Show details\",\n",
        "    \"Show me the\", \"List the\", \"Fetch details\", \"Get information\",\n",
        "    \"Provide details\", \"Find details\", \"Retrieve data\", \"Show information\",\n",
        "    \"Display details\", \"Display all\", \"Get list\", \"Show records\",\n",
        "    \"Retrieve list\", \"Find all\", \"Get data\", \"List records\",\n",
        "    \"Provide records\", \"Fetch records\", \"Show data\", \"Find records\",\n",
        "    \"Show items\", \"Retrieve items\", \"Display items\", \"List items\",\n",
        "    \"Get items\", \"Show info\", \"Fetch info\", \"Find info\",\n",
        "    \"Display info\", \"List info\", \"Provide info\", \"Show entry\",\n",
        "    \"Retrieve entry\", \"Display entry\", \"List entry\", \"Get entry\",\n",
        "    \"Find entry\", \"Show output\", \"Fetch output\", \"Retrieve output\",\n",
        "    \"Display output\", \"List output\", \"Get output\", \"Find output\",\n",
        "    \"Show list\", \"Retrieve list\", \"Display list\", \"Fetch list\",\n",
        "    \"List info\", \"Show data\", \"Get details\", \"Display records\",\n",
        "    \"Find data\", \"Retrieve records\", \"Show complete\", \"Fetch complete\",\n",
        "    \"List complete\", \"Get complete\", \"Find complete\", \"Show overview\",\n",
        "    \"Retrieve overview\", \"Display overview\", \"List overview\", \"Get overview\",\n",
        "    \"Find overview\", \"Show summary\", \"Fetch summary\", \"Retrieve summary\",\n",
        "    \"Display summary\", \"List summary\", \"Get summary\", \"Find summary\",\n",
        "    \"Show all\", \"Retrieve all\", \"Display all\", \"List all\", \"Get all\",\n",
        "    \"each\", \"every\", \"all\", \"both\", \"either\", \"neither\", \"one\", \"none\",\n",
        "    \"some\", \"any\", \"much\", \"more\", \"most\", \"few\", \"several\", \"couple\",\n",
        "    \"every\", \"every\", \"no\", \"nothing\", \"everything\", \"something\", \"anything\",\n",
        "    \"each\", \"every\", \"many\", \"few\", \"several\", \"another\", \"last\", \"next\",\n",
        "    \"first\", \"last\", \"second\", \"third\", \"few\", \"more\", \"most\", \"less\",\n",
        "    \"most\", \"least\", \"every\", \"every\", \"where\", \"somewhere\", \"anywhere\",\n",
        "    \"nowhere\", \"here\", \"there\", \"near\", \"far\", \"inside\", \"outside\", \"above\",\n",
        "    \"below\", \"underneath\", \"over\", \"beneath\", \"between\", \"among\", \"within\",\n",
        "    \"around\", \"throughout\", \"beside\", \"alongside\", \"toward\", \"backward\",\n",
        "    \"forward\", \"upward\", \"downward\", \"side\", \"left\", \"right\", \"straight\",\n",
        "    \"beyond\", \"close\", \"close\", \"away\", \"inside\", \"outside\", \"along\",\n",
        "    \"against\", \"towards\", \"amongst\", \"amidst\", \"afterwards\", \"beforehand\",\n",
        "    \"hereafter\", \"thereafter\", \"earlier\", \"later\", \"above\", \"below\",\n",
        "    \"up\", \"down\", \"under\", \"over\", \"underneath\", \"beneath\", \"over\",\n",
        "    \"under\", \"between\", \"among\", \"between\", \"toward\", \"from\", \"by\",\n",
        "    \"upon\", \"according\", \"except\", \"outside\", \"beside\", \"along\",\n",
        "    \"through\", \"until\", \"during\", \"before\", \"after\", \"while\", \"if\",\n",
        "    \"unless\", \"as\", \"unless\", \"until\", \"since\", \"until\", \"once\",\n",
        "    \"if\", \"provided\", \"unless\", \"even\", \"though\", \"although\", \"unless\",\n",
        "    \"whether\", \"as\", \"like\", \"than\", \"when\", \"where\", \"how\", \"why\",\n",
        "    \"what\", \"which\", \"who\", \"whom\", \"whose\", \"that\", \"this\", \"these\",\n",
        "    \"those\", \"another\", \"other\", \"some\", \"any\", \"much\", \"more\",\n",
        "    \"many\", \"few\", \"several\", \"both\", \"either\", \"neither\", \"such\",\n",
        "    \"same\", \"different\", \"better\", \"worse\", \"more\", \"less\", \"least\",\n",
        "    \"most\", \"more\", \"few\", \"fewer\", \"less\", \"last\", \"next\", \"recent\",\n",
        "    \"old\", \"young\", \"new\", \"early\", \"late\", \"recent\", \"past\", \"future\",\n",
        "    \"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"by\", \"with\", \"of\", \"for\",\n",
        "    \"to\", \"from\", \"about\", \"under\", \"over\", \"between\", \"among\", \"near\",\n",
        "    \"through\", \"within\", \"around\", \"beside\", \"behind\", \"after\", \"before\",\n",
        "    \"during\", \"past\", \"since\", \"until\", \"as\", \"like\", \"so\", \"but\",\n",
        "    \"or\", \"and\", \"nor\", \"if\", \"then\", \"else\", \"when\", \"where\", \"how\",\n",
        "    \"why\", \"which\", \"what\", \"who\", \"whom\", \"whose\", \"that\", \"this\",\n",
        "    \"these\", \"those\", \"one\", \"ones\", \"some\", \"any\", \"each\", \"every\",\n",
        "    \"all\", \"many\", \"few\", \"several\", \"more\", \"most\", \"less\", \"least\",\n",
        "    \"much\", \"just\", \"only\", \"also\", \"very\", \"such\", \"as\", \"than\", \"than\",\n",
        "    \"both\", \"each\", \"either\", \"neither\", \"another\", \"few\", \"several\",\n",
        "    \"many\", \"much\", \"one\", \"each\", \"every\", \"some\", \"any\", \"more\",\n",
        "    \"most\", \"less\", \"least\", \"few\", \"little\", \"several\", \"several\",\n",
        "    \"couple\", \"do\", \"does\", \"did\", \"has\", \"have\", \"had\", \"will\",\n",
        "    \"shall\", \"would\", \"could\", \"should\", \"might\", \"may\", \"can\", \"must\",\n",
        "    \"need\", \"ought\", \"like\", \"seem\", \"seems\", \"appears\", \"show\", \"make\",\n",
        "    \"keep\", \"come\", \"go\", \"get\", \"give\", \"take\", \"bring\", \"send\",\n",
        "    \"reply\", \"return\", \"make\", \"make\", \"allow\", \"ask\", \"want\",\n",
        "    \"need\", \"prefer\", \"try\", \"start\", \"stop\", \"finish\", \"continue\", \"turn\",\n",
        "    \"change\", \"move\", \"stay\", \"remain\", \"stay\", \"keep\", \"hold\", \"wait\",\n",
        "    \"look\", \"see\", \"hear\", \"find\", \"find\", \"lose\", \"search\", \"check\",\n",
        "    \"find\", \"search\", \"show\", \"find\", \"find\", \"describe\", \"explain\",\n",
        "    \"give\", \"provide\", \"offer\", \"request\", \"query\", \"ask\", \"answer\",\n",
        "    \"resolve\", \"solve\", \"complete\", \"complete\", \"fix\", \"solve\", \"resolve\",\n",
        "    \"show\", \"find\", \"give\", \"provide\", \"offer\", \"describe\", \"explain\",\n",
        "    \"request\", \"query\", \"ask\", \"answer\", \"resolve\", \"solve\", \"complete\",\n",
        "    '?','!','.'\n",
        "]\n",
        "\n",
        "# Creating training sets\n",
        "x_train_tokens = []\n",
        "x_train_tokens.extend(Eq_ID)\n",
        "x_train_tokens.extend(Eq_Name)\n",
        "x_train_tokens.extend(Loc_Hardware)\n",
        "x_train_tokens.extend(S_No)\n",
        "x_train_tokens.extend(SR_No)\n",
        "x_train_tokens.extend(Asset_ID)\n",
        "# x_train_tokens.extend(Col_Name)\n",
        "x_train_tokens.extend(General)\n",
        "\n",
        "# Corresponding labels\n",
        "y_train_tokens = [\"Equipment ID\"] * len(Eq_ID) + [\"Name of Equipment\"] * len(Eq_Name) + [\"Location (Hardware)\"] * len(Loc_Hardware) + [\"Serial No.\"] * len(S_No) + [\"Store Ref. No.\"] * len(SR_No) + [\"Asset ID\"] * len(Asset_ID) + [\"General\"] * len(General) #+ [\"Column Name\"] * len(Col_Name)\n",
        "\n",
        "print (len(y_train_tokens))\n",
        "print (len(x_train_tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5Tm5q1YklLd",
        "outputId": "0e11ddd9-f0d7-40e4-d006-98914f507cde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1141\n",
            "1141\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating token embeddings for training\n",
        "x_train_token_embeddings = [preprocess_query(token).mean(dim=1).detach().numpy()[0] for token in x_train_tokens]\n",
        "\n",
        "# Train the token classification model\n",
        "token_classifier = RandomForestClassifier()\n",
        "token_classifier.fit(x_train_token_embeddings, y_train_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "yWppToNV3NYR",
        "outputId": "e1c057c3-5a2d-46f9-bf1e-067cd5c51c82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier()"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Loading spacy for nlp operations\n",
        "# nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# # Training the token classification data\n",
        "# token_data = {\"token\": x_train_tokens, \"label\": y_train_tokens}\n",
        "# # Convert to DataFrame\n",
        "# df_tokens = pd.DataFrame(token_data)\n",
        "\n",
        "# nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# # Function to extract features from a token\n",
        "# def extract_features(token):\n",
        "#     doc = nlp(token)\n",
        "#     features = {\n",
        "#         \"text\": token,\n",
        "#         \"pos\": doc[0].pos_,\n",
        "#         \"dep\": doc[0].dep_\n",
        "#     }\n",
        "#     return features\n",
        "\n",
        "# # Extract features for each token\n",
        "# features = [extract_features(row[\"token\"]) for _, row in df_tokens.iterrows()]\n",
        "\n",
        "# # Convert features to DataFrame\n",
        "# df_features = pd.DataFrame(features)\n",
        "\n",
        "# # Vectorize the features\n",
        "# vectorizer = CountVectorizer()\n",
        "# X = vectorizer.fit_transform(df_features[\"text\"])\n",
        "# y = df_tokens[\"label\"]\n",
        "\n",
        "# # Train RandomForestClassifier\n",
        "# clf = RandomForestClassifier()\n",
        "# clf.fit(X, y)"
      ],
      "metadata": {
        "id": "urGsqsjmk92e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTokenizer:\n",
        "    def __init__(self, multi_word_tokens):\n",
        "        self.multi_word_tokens = sorted(multi_word_tokens, key=len, reverse=True)  # Sort by length for longest match first\n",
        "        self.token_pattern = re.compile(r'\\b(?:' + '|'.join(re.escape(token) for token in self.multi_word_tokens) + r')\\b')\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        tokens = []\n",
        "        last_index = 0\n",
        "\n",
        "        for match in self.token_pattern.finditer(text):\n",
        "            start, end = match.span()\n",
        "            if start > last_index:\n",
        "                non_matching_text = text[last_index:start].split()\n",
        "                tokens.extend(non_matching_text)\n",
        "            tokens.append(match.group())\n",
        "            last_index = end\n",
        "\n",
        "        if last_index < len(text):\n",
        "            tokens.extend(text[last_index:].split())\n",
        "\n",
        "        return tokens\n",
        "\n",
        "# Example multi-word tokens\n",
        "data01 = pd.read_excel('multiwordlist.xlsx')\n",
        "df01 = pd.DataFrame(data01)\n",
        "multi_word_tokens = df01.iloc[:, 0].tolist()\n",
        "print(multi_word_tokens)\n",
        "tokenizer = CustomTokenizer(multi_word_tokens)\n"
      ],
      "metadata": {
        "id": "Zr0-dp7v4S3X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1187938-410b-489f-b45c-955b6cfbe45d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Replacement for STC ', 'Replacement STC ', 'STC Replacement ', 'Station Computer-01 DELL-T5600', 'Station Computer-01 DELL-T5600', 'Station Computer-01 ', 'Station Computer-03 DELL-T5820 ', 'Station Computer-03 ', 'GEI - SPARE BECKHOFF/C5101-0010 ', 'GEI - SPARE BECKHOFF', 'RKVM-1501 RS_400CD_US', 'HP LaserJet Printer P3015', 'HP LaserJet Printer ', 'NCC Router-02 ', 'NCC Router ', 'HP Compaq pro 6300', 'VHF Computer ', 'UHF Computer ', 'DELL Precision 5820', 'Ethernet Switch', '24V/28V DC Power Supply', 'Power Supply', 'Thinvent Thinclient ', 'Station Computer-04 DELL-T5900', 'Station Computer-04 ', 'GEI - SPARE BECKHOFF/C5102-0011 ', 'GEI - SPARE BECKHOFF', 'RKVM-1502 RS_400CD_US', 'HP LaserJet Printer P3016 ', 'HP LaserJet Printer', 'NCC Router-03', 'NCC Router', 'HP Compaq pro 6301 ', 'HP Compaq pro 6301 ', 'VHF Radio', 'UHF Radio', 'DELL Precision 5830 ', 'Ethernet Switch Pro', 'Comp Lab ', 'EAST ANT ROOM ', 'WEST ANT ROOM ', 'SPARE', 'IRDCN ROOM ', 'Computer Lab ', 'IRDCN ROOM ', 'Equipment ID ', 'Name of Equipment ', 'Location ', 'Serial No.', 'Store Ref. No. ', 'Asset ID ', 'Software ID ', 'Software Name', 'Location (Software) ', 'Revision No. ', 'Date of Updation', 'General ', 'Equipment ID ', 'Name of Equipment', 'Location (Hardware) ', 'Serial No. ', 'Store Ref. No.', 'Asset ID ', 'Software ID ', 'Software Name ', 'Location (Software) ', 'Revision No. ', 'Date of Updation']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load spaCy's pre-trained model for NER\n",
        "# nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# class CustomDynamicTokenizer:\n",
        "#     def __init__(self, nlp):\n",
        "#         self.nlp = nlp\n",
        "\n",
        "#     def tokenize(self, text):\n",
        "#         doc = self.nlp(text)\n",
        "#         tokens = []\n",
        "#         for ent in doc.ents:\n",
        "#             tokens.append(ent.text)\n",
        "#         for token in doc:\n",
        "#             if not token.ent_type_:\n",
        "#                 tokens.append(token.text)\n",
        "#         return tokens\n",
        "\n",
        "# # Example usage\n",
        "# tokenizer = CustomDynamicTokenizer(nlp)"
      ],
      "metadata": {
        "id": "slKMlrBcezto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Where is stn_comp-01 located?\"\n",
        "\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "embedding_model = AutoModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Preprocess query to get embeddings\n",
        "def preprocessing_query(query):\n",
        "    inputs = bert_tokenizer(query, return_tensors='pt')\n",
        "    outputs = embedding_model(**inputs)\n",
        "    return outputs.last_hidden_state\n",
        "\n",
        "# Function to classify tokens\n",
        "def classify_tokens(query):\n",
        "    # Tokenize using the custom tokenizer\n",
        "    tokens = tokenizer.tokenize(query)\n",
        "\n",
        "    # Generate embeddings for each token using BERT\n",
        "    token_embeddings = []\n",
        "    for token in tokens:\n",
        "        embedding = preprocessing_query(token).mean(dim=1).detach().numpy()[0]\n",
        "        token_embeddings.append(embedding)\n",
        "\n",
        "    # Convert token_embeddings to a numpy array and reshape if necessary\n",
        "    token_embeddings = np.array(token_embeddings)\n",
        "    if len(token_embeddings.shape) == 1:\n",
        "        token_embeddings = token_embeddings.reshape(1, -1)\n",
        "\n",
        "    # Classify tokens using the trained model\n",
        "    classifications = token_classifier.predict(token_embeddings)\n",
        "\n",
        "    return list(zip(tokens, classifications))\n",
        "\n",
        "# Test the classifier with a query\n",
        "# query = \"Where is equipment name NOP/LGS/STC/02, located in Stn_Comp-01 DELL-T5600, for HP_Compaq_pro_6300, Power_Supply, f88ss6y40 and Beckhoff-ethercat?\"\n",
        "classified_tokens = classify_tokens(query)\n",
        "\n",
        "# Display results\n",
        "print(\"Classified Tokens:\")\n",
        "for token, label in classified_tokens:\n",
        "    print(f\"Token: {token}, Label: {label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXdSOT-Ulb10",
        "outputId": "a5e48565-3068-440f-b1de-dc988976caea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classified Tokens:\n",
            "Token: Where, Label: General\n",
            "Token: is, Label: General\n",
            "Token: stn_comp-01, Label: Name of Equipment\n",
            "Token: located?, Label: General\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load spaCy's pre-trained NER model\n",
        "# nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# def apply_ner_to_classified_tokens(classified_tokens):\n",
        "#     ner_results = []\n",
        "#     for token, label in classified_tokens:\n",
        "#         if label in ['Name of Equipment', 'Equipment ID']:  # Apply NER only to relevant labels\n",
        "#             doc = nlp(token)\n",
        "#             entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "#             ner_results.append((token, label, entities))\n",
        "#         else:\n",
        "#             ner_results.append((token, label, []))\n",
        "#     return ner_results\n",
        "\n",
        "# # Example usage\n",
        "# ner_results = apply_ner_to_classified_tokens(classified_tokens)\n",
        "\n",
        "# # Display results\n",
        "# for token, label, entities in ner_results:\n",
        "#     print(f\"Token: {token}, Label: {label}, Entities: {entities}\")\n"
      ],
      "metadata": {
        "id": "Tl3LpIBsnSA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install contractions"
      ],
      "metadata": {
        "id": "hppstCjPAjAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import contractions\n",
        "\n",
        "# def expand_contractions(text):\n",
        "#     return contractions.fix(text)\n",
        "\n",
        "# # Example usage\n",
        "# text = \"I can't go to stn. Cuz my comp is fried.\"\n",
        "# expanded_text = expand_contractions(text)\n",
        "# print(expanded_text)"
      ],
      "metadata": {
        "id": "BciHVYrZW6jO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import nltk\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('wordnet')\n",
        "# from nltk.corpus import wordnet\n",
        "# from nltk.tokenize import word_tokenize\n",
        "\n",
        "# def get_synonyms(word):\n",
        "#     synonyms = set()\n",
        "#     for syn in wordnet.synsets(word):\n",
        "#         for lemma in syn.lemmas():\n",
        "#             synonyms.add(lemma.name())\n",
        "#     return synonyms\n",
        "\n",
        "# def standardize_synonyms(text):\n",
        "#     tokens = word_tokenize(text)\n",
        "#     standardized_tokens = []\n",
        "#     for token in tokens:\n",
        "#         synonyms = get_synonyms(token)\n",
        "#         if synonyms:\n",
        "#             # Choose the first synonym (you might want a better strategy)\n",
        "#             standardized_tokens.append(list(synonyms)[0])\n",
        "#         else:\n",
        "#             standardized_tokens.append(token)\n",
        "#     return ' '.join(standardized_tokens)\n",
        "\n",
        "# # Example usage\n",
        "# text = \"Find the asset number for the equip ID\"\n",
        "# standardized_text = standardize_synonyms(text)\n",
        "# print(standardized_text)"
      ],
      "metadata": {
        "id": "Cyg7CNpqAavH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import openai\n",
        "\n",
        "# # Function to call GPT-3/4 for intention recognition and token classification\n",
        "# def analyze_query(query):\n",
        "#     # Initial prompt with examples\n",
        "#     prompt = \"\"\"\n",
        "#     You are an AI designed to interpret natural language queries for database searches. Your task is to:\n",
        "#     1. Identify the table to be searched.\n",
        "#     2. Classify tokens in the query to identify relevant columns.\n",
        "#     3. Normalize the query for consistent terminology.\n",
        "\n",
        "#     Here are some examples:\n",
        "\n",
        "#     Query: \"Where is equipment name A00020210098 located?\"\n",
        "#     - Table: equipment\n",
        "#     - Columns: [name, location]\n",
        "#     - Normalized Query: \"SELECT location FROM equipment WHERE name = 'A00020210098'\"\n",
        "\n",
        "#     Query: \"Show me the software details for ID SFT12345.\"\n",
        "#     - Table: software\n",
        "#     - Columns: [ID, details]\n",
        "#     - Normalized Query: \"SELECT details FROM software WHERE ID = 'SFT12345'\"\n",
        "\n",
        "#     Now, analyze the following query and provide the table, columns, and normalized query:\n",
        "#     \"\"\"\n",
        "\n",
        "#     # Add the user's query to the prompt\n",
        "#     prompt += f\"Query: \\\"{query}\\\"\\n\"\n",
        "\n",
        "#     # Call the OpenAI API\n",
        "#     response = openai.Completion.create(\n",
        "#         engine=\"text-davinci-003\",  # or the latest model available\n",
        "#         prompt=prompt,\n",
        "#         max_tokens=150,\n",
        "#         n=1,\n",
        "#         stop=None,\n",
        "#         temperature=0.5,\n",
        "#     )\n",
        "\n",
        "#     # Parse the response\n",
        "#     output = response.choices[0].text.strip()\n",
        "#     return output\n",
        "\n",
        "# # Example usage\n",
        "# query = \"Where is equipment name A00020210098 located?\"\n",
        "# result = analyze_query(query)\n",
        "# print(result)"
      ],
      "metadata": {
        "id": "Jirs9k86A1Nd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers"
      ],
      "metadata": {
        "id": "Mpp8B6F3NUxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments"
      ],
      "metadata": {
        "id": "E421WEU2PVlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_name = 'gpt2-large'#\"gpt2\"  # or 'gpt2-medium', 'gpt2-large', etc.\n",
        "# tokenizerX = GPT2Tokenizer.from_pretrained(model_name)\n",
        "# modelX = GPT2LMHeadModel.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "id": "NejosO1jPbH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initial_promp = \"You are an AI model trained to generate synonyms and abbreviations for a given word. If a text is given parse the text into individual words and output the abbreviations and synonyms. The word is: \""
      ],
      "metadata": {
        "id": "IU472PzOPoDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# User input query\n",
        "# user_query = \"Station Computer 01\"\n",
        "\n",
        "# Combine initial prompt and user input\n",
        "# input_text = initial_prompt\n",
        "\n",
        "# # Tokenize input text\n",
        "# input_ids = tokenizerX.encode(input_text, return_tensors='pt')\n",
        "\n",
        "# # Generate output\n",
        "# output = modelX.generate(input_ids, max_length=1000, num_return_sequences=1, no_repeat_ngram_size=2)\n",
        "\n",
        "# # Decode and print output\n",
        "# normalized_query = tokenizerX.decode(output[0], skip_special_tokens=True)\n",
        "# print(\"Normalized Query:\", normalized_query)\n"
      ],
      "metadata": {
        "id": "I2Wh7G9LQPt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "abXKcddzcJfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "import textwrap\n",
        "\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Used to securely store your API key\n",
        "from google.colab import userdata\n",
        "\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "\n",
        "def to_markdown(text):\n",
        "  text = text.replace('•', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
      ],
      "metadata": {
        "id": "zYl482HGc-HV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Or use `os.getenv('GOOGLE_API_KEY')` to fetch an environment variable.\n",
        "GOOGLE_API_KEY=userdata.get('gemini')\n",
        "\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "9aJDCeqIXH9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for m in genai.list_models():\n",
        "  if 'generateContent' in m.supported_generation_methods:\n",
        "    print(m.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "vnPyshzCYddj",
        "outputId": "9d8cfeb5-2f30-49cd-9176-a3d6cf1cf16e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/gemini-1.0-pro-latest\n",
            "models/gemini-1.0-pro\n",
            "models/gemini-pro\n",
            "models/gemini-1.0-pro-001\n",
            "models/gemini-1.0-pro-vision-latest\n",
            "models/gemini-pro-vision\n",
            "models/gemini-1.5-pro-latest\n",
            "models/gemini-1.5-pro-001\n",
            "models/gemini-1.5-pro\n",
            "models/gemini-1.5-flash-latest\n",
            "models/gemini-1.5-flash-001\n",
            "models/gemini-1.5-flash\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modelY = genai.GenerativeModel('gemini-1.5-pro')"
      ],
      "metadata": {
        "id": "kMbPeJY7dN4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mapping (token_word):\n",
        "    mappings = [\"Station_Computer_01\", \"Equipment_ID\", \"Equipment\", \"Telemetry\", \"Station_Computer_02\", \"EAST_ANT_ROOM\", \"28V/24V_DC_Power_Supply\", \"West_Ant_Room\"]\n",
        "    ambiguous_word = token_word\n",
        "    mapping_prompt = f\"\"\"\n",
        "      You are an AI that helps to find suitable mappings for ambiguous words in database queries.\n",
        "      Given a list of possible mappings, identify the most appropriate mapping for the provided ambiguous word. The ambiguous word may be an abbreviation or may include synonym of the mapping word.\n",
        "      The list of possible mappings is as follows:\n",
        "      {mappings}\n",
        "\n",
        "      For example:\n",
        "      Ambiguous Word: \"equip\"\n",
        "      Suitable Mapping: \"equipment\"\n",
        "\n",
        "      Ambiguous Word: \"Telemet\"\n",
        "      Suitable Mapping: \"Telemetry\"\n",
        "\n",
        "      Find the suitable mapping for:\n",
        "      Ambiguous Word: \"{ambiguous_word}\"\n",
        "      Suitable Mapping:\n",
        "\n",
        "      Just give the  suitable mapping word(s) (separated by \"and\" if more than one). No need of explanation.\n",
        "      If no suitable or misleading mapping found output text: \"No suitable mapping found\".\n",
        "      \"\"\"\n",
        "    # %%time\n",
        "    response = modelY.generate_content(mapping_prompt, stream=True)\n",
        "\n",
        "    # for chunk in response:\n",
        "    #   print(chunk.text)\n",
        "\n",
        "    # Initialize an empty string to hold the complete response\n",
        "    complete_response = \"\"\n",
        "\n",
        "    # Concatenate all chunks into the complete response string\n",
        "    for chunk in response:\n",
        "        complete_response += chunk.text\n",
        "\n",
        "    # Print the complete response\n",
        "    print(complete_response)\n",
        "    if(complete_response == \"No suitable mapping found\"):\n",
        "      return ambiguous_word\n",
        "    else:\n",
        "      return complete_response"
      ],
      "metadata": {
        "id": "94fcYQ9qlkwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "re_query = \"\";\n",
        "label_token = {}\n",
        "for token, label in classified_tokens:\n",
        "    if (label != \"General\"):\n",
        "        new_token = mapping(token)\n",
        "        # print(token)\n",
        "        label_token.update({label:new_token})\n",
        "        re_query = re_query + \" \" + new_token\n",
        "    else:\n",
        "        re_query = re_query + \" \" + token\n",
        "print(re_query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "fXn5GhuPdgCN",
        "outputId": "67c346a1-608f-44f9-a8f2-765773015c3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Station_Computer_01 \n",
            "\n",
            " Where is Station_Computer_01 \n",
            " located?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "table = \"Equipment\"\n",
        "# intent, confidence = recognize_intent(re_query, intent_model)\n",
        "# print(f\"Intent recognized: {intent}, Confidence level: {confidence:.2f}\")\n",
        "# if (intent == 0):\n",
        "#     table = \"Equipment\"\n",
        "# else:\n",
        "#     table = \"Software\""
      ],
      "metadata": {
        "id": "ULjn5l9yOwhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelX = genai.GenerativeModel('gemini-1.5-pro-latest')"
      ],
      "metadata": {
        "id": "ZwrhJJ8EeFZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_generator_prompt = f\"\"\"\n",
        "    You are an AI that generates SQL queries from a given natural language query.\n",
        "    The table data structure is as follows:\n",
        "    Table-1: Equipments Table -\n",
        "    Column-1: Equipment ID\n",
        "    Column-2: Equipment Name\n",
        "    Column-3: Location (Hardware)\n",
        "    Column-4: Serial No.\n",
        "    Column-5: Store Ref. No.\n",
        "    Column-6: Asset ID\n",
        "\n",
        "    Some guidelines to form the query are:\n",
        "    1. SQL Query structure must start with SELECT.\n",
        "    2. The natural language query has words separated by spaces.\n",
        "    3. The natural language query, table name and the possible key value pairs of column_name and specific_records will be provided to you.\n",
        "\n",
        "    Provided information:\n",
        "    1. Natural Language Query: {re_query}\n",
        "    2. Table name: {table}\n",
        "    3. Column name : Specific_records :: {label_token}\n",
        "\n",
        "    Give the natural language query and the SQL query only. No explanation needed.\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "JO2wzTzZvPP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "response = modelX.generate_content(query_generator_prompt, stream=True)\n",
        "\n",
        "# for chunk in response:\n",
        "#   print(chunk.text)\n",
        "\n",
        "# Initialize an empty string to hold the complete response\n",
        "complete_response = \"\"\n",
        "\n",
        "# Concatenate all chunks into the complete response string\n",
        "for chunk in response:\n",
        "    complete_response += chunk.text\n",
        "\n",
        "# Print the complete response\n",
        "print(complete_response)"
      ],
      "metadata": {
        "id": "141CRPWfwg85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "e9f7d33b-0871-43ce-f27b-2311c471c19b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural Language Query: Where is Station_Computer_01 located? \n",
            "SQL Query: SELECT Location FROM Equipment WHERE `Name of Equipment` = 'Station_Computer_01'\n",
            "CPU times: user 75.4 ms, sys: 5.2 ms, total: 80.6 ms\n",
            "Wall time: 4.52 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q6XD6OC-UM2F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}